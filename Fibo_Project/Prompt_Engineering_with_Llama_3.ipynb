{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/meta-llama/llama-recipes/blob/main/recipes/quickstart/Prompt_Engineering_with_Llama_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Prompt Engineering with Llama 3.1\n",
    "\n",
    "Prompt engineering is using natural language to produce a desired response from a large language model (LLM).\n",
    "\n",
    "This interactive guide covers prompt engineering & best practices with Llama 3.1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why now?\n",
    "\n",
    "[Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762) introduced the world to transformer neural networks (originally for machine translation). Transformers ushered an era of generative AI with diffusion models for image creation and large language models (`LLMs`) as **programmable deep learning networks**.\n",
    "\n",
    "Programming foundational LLMs is done with natural language – it doesn't require training/tuning like ML models of the past. This has opened the door to a massive amount of innovation and a paradigm shift in how technology can be deployed. The science/art of using natural language to program language models to accomplish a task is referred to as **Prompt Engineering**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama Models\n",
    "\n",
    "In 2023, Meta introduced the [Llama language models](https://ai.meta.com/llama/) (Llama Chat, Code Llama, Llama Guard). These are general purpose, state-of-the-art LLMs.\n",
    "\n",
    "Llama models come in varying parameter sizes. The smaller models are cheaper to deploy and run; the larger models are more capable.\n",
    "\n",
    "#### Llama 3.1\n",
    "1. `llama-3.1-8b` - base pretrained 8 billion parameter model\n",
    "1. `llama-3.1-70b` - base pretrained 70 billion parameter model\n",
    "1. `llama-3.1-405b` - base pretrained 405 billion parameter model\n",
    "1. `llama-3.1-8b-instruct` - instruction fine-tuned 8 billion parameter model\n",
    "1. `llama-3.1-70b-instruct` - instruction fine-tuned 70 billion parameter model\n",
    "1. `llama-3.1-405b-instruct` - instruction fine-tuned 405 billion parameter model (flagship)\n",
    "\n",
    "\n",
    "#### Llama 3\n",
    "1. `llama-3-8b` - base pretrained 8 billion parameter model\n",
    "1. `llama-3-70b` - base pretrained 70 billion parameter model\n",
    "1. `llama-3-8b-instruct` - instruction fine-tuned 8 billion parameter model\n",
    "1. `llama-3-70b-instruct` - instruction fine-tuned 70 billion parameter model (flagship)\n",
    "\n",
    "#### Llama 2\n",
    "1. `llama-2-7b` - base pretrained 7 billion parameter model\n",
    "1. `llama-2-13b` - base pretrained 13 billion parameter model\n",
    "1. `llama-2-70b` - base pretrained 70 billion parameter model\n",
    "1. `llama-2-7b-chat` - chat fine-tuned 7 billion parameter model\n",
    "1. `llama-2-13b-chat` - chat fine-tuned 13 billion parameter model\n",
    "1. `llama-2-70b-chat` - chat fine-tuned 70 billion parameter model (flagship)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Llama is a code-focused LLM built on top of Llama 2 also available in various sizes and finetunes:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Llama\n",
    "1. `codellama-7b` - code fine-tuned 7 billion parameter model\n",
    "1. `codellama-13b` - code fine-tuned 13 billion parameter model\n",
    "1. `codellama-34b` - code fine-tuned 34 billion parameter model\n",
    "1. `codellama-70b` - code fine-tuned 70 billion parameter model\n",
    "1. `codellama-7b-instruct` - code & instruct fine-tuned 7 billion parameter model\n",
    "2. `codellama-13b-instruct` - code & instruct fine-tuned 13 billion parameter model\n",
    "3. `codellama-34b-instruct` - code & instruct fine-tuned 34 billion parameter model\n",
    "3. `codellama-70b-instruct` - code & instruct fine-tuned 70 billion parameter model\n",
    "1. `codellama-7b-python` - Python fine-tuned 7 billion parameter model\n",
    "2. `codellama-13b-python` - Python fine-tuned 13 billion parameter model\n",
    "3. `codellama-34b-python` - Python fine-tuned 34 billion parameter model\n",
    "3. `codellama-70b-python` - Python fine-tuned 70 billion parameter model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting an LLM\n",
    "\n",
    "Large language models are deployed and accessed in a variety of ways, including:\n",
    "\n",
    "1. **Self-hosting**: Using local hardware to run inference. Ex. running Llama on your Macbook Pro using [llama.cpp](https://github.com/ggerganov/llama.cpp).\n",
    "    * Best for privacy/security or if you already have a GPU.\n",
    "1. **Cloud hosting**: Using a cloud provider to deploy an instance that hosts a specific model. Ex. running Llama on cloud providers like AWS, Azure, GCP, and others.\n",
    "    * Best for customizing models and their runtime (ex. fine-tuning a model for your use case).\n",
    "1. **Hosted API**: Call LLMs directly via an API. There are many companies that provide Llama inference APIs including AWS Bedrock, Replicate, Anyscale, Together and others.\n",
    "    * Easiest option overall."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hosted APIs\n",
    "\n",
    "Hosted APIs are the easiest way to get started. We'll use them here. There are usually two main endpoints:\n",
    "\n",
    "1. **`completion`**: generate a response to a given prompt (a string).\n",
    "1. **`chat_completion`**: generate the next message in a list of messages, enabling more explicit instruction and context for use cases like chatbots."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "\n",
    "LLMs process inputs and outputs in chunks called *tokens*. Think of these, roughly, as words – each model will have its own tokenization scheme. For example, this sentence...\n",
    "\n",
    "> Our destiny is written in the stars.\n",
    "\n",
    "...is tokenized into `[\"Our\", \" destiny\", \" is\", \" written\", \" in\", \" the\", \" stars\", \".\"]` for Llama 3. See [this](https://tiktokenizer.vercel.app/?model=meta-llama%2FMeta-Llama-3-8B) for an interactive tokenizer tool.\n",
    "\n",
    "Tokens matter most when you consider API pricing and internal behavior (ex. hyperparameters).\n",
    "\n",
    "Each model has a maximum context length that your prompt cannot exceed. That's 128k tokens for Llama 3.1, 4K for Llama 2, and 100K for Code Llama.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "\n",
    "The following APIs will be used to call LLMs throughout the guide. As an example, we'll call Llama 3.1 chat using [Grok](https://console.groq.com/playground?model=llama3-70b-8192).\n",
    "\n",
    "To install prerequisites run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List\n",
    "from groq import Groq\n",
    "\n",
    "# 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\n",
    "# Get a free API key from https://console.groq.com/keys\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "LLAMA3_405B_INSTRUCT = \"llama-3.1-405b-reasoning\" # Note: Groq currently only gives access here to paying customers for 405B model\n",
    "LLAMA3_70B_INSTRUCT = \"llama-3.1-70b-versatile\"\n",
    "LLAMA3_8B_INSTRUCT = \"llama3.1-8b-instant\"\n",
    "\n",
    "DEFAULT_MODEL = LLAMA3_70B_INSTRUCT\n",
    "\n",
    "client = Groq()\n",
    "\n",
    "def assistant(content: str):\n",
    "    return { \"role\": \"assistant\", \"content\": content }\n",
    "\n",
    "def user(content: str):\n",
    "    return { \"role\": \"user\", \"content\": content }\n",
    "\n",
    "def chat_completion(\n",
    "    messages: List[Dict],\n",
    "    model = DEFAULT_MODEL,\n",
    "    temperature: float = 0.6,\n",
    "    top_p: float = 0.9,\n",
    ") -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "        \n",
    "\n",
    "def completion(\n",
    "    prompt: str,\n",
    "    model: str = DEFAULT_MODEL,\n",
    "    temperature: float = 0.6,\n",
    "    top_p: float = 0.9,\n",
    ") -> str:\n",
    "    return chat_completion(\n",
    "        [user(prompt)],\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "def complete_and_print(prompt: str, model: str = DEFAULT_MODEL):\n",
    "    print(f'==============\\nAssistant --> {prompt}\\n==============')\n",
    "    response = completion(prompt, model)\n",
    "    print(f'user --> {response}', end='\\n\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion APIs\n",
    "\n",
    "Let's try Llama 3.1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "The typical color of the sky is: \n",
      "==============\n",
      "Blue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"The typical color of the sky is: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "which model version are you?\n",
      "==============\n",
      "I'm a large language model, my version is InstructLLaMA, but for the purposes of this conversation, I'll refer to my knowledge cutoff as of \"December 2023\" but keep in mind my training data only goes up to 2022.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"which model version are you?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Completion APIs\n",
    "Chat completion models provide additional structure to interacting with an LLM. An array of structured message objects is sent to the LLM instead of a single piece of text. This message list provides the LLM with some \"context\" or \"history\" from which to continue.\n",
    "\n",
    "Typically, each message contains `role` and `content`:\n",
    "* Messages with the `system` role are used to provide core instruction to the LLM by developers.\n",
    "* Messages with the `user` role are typically human-provided messages.\n",
    "* Messages with the `assistant` role are typically generated by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your favorite color is blue.\n"
     ]
    }
   ],
   "source": [
    "response = chat_completion(messages=[\n",
    "    user(\"My favorite color is blue.\"),\n",
    "    assistant(\"That's great to hear!\"),\n",
    "    user(\"What is my favorite color?\"),\n",
    "])\n",
    "print(response)\n",
    "# \"Sure, I can help you with that! Your favorite color is blue.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Hyperparameters\n",
    "\n",
    "#### `temperature` & `top_p`\n",
    "\n",
    "These APIs also take parameters which influence the creativity and determinism of your output.\n",
    "\n",
    "At each step, LLMs generate a list of most likely tokens and their respective probabilities. The least likely tokens are \"cut\" from the list (based on `top_p`), and then a token is randomly selected from the remaining candidates (`temperature`).\n",
    "\n",
    "In other words: `top_p` controls the breadth of vocabulary in a generation and `temperature` controls the randomness within that vocabulary. A temperature of ~0 produces *almost* deterministic results.\n",
    "\n",
    "[Read more about temperature setting here](https://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api-a-few-tips-and-tricks-on-controlling-the-creativity-deterministic-output-of-prompt-responses/172683).\n",
    "\n",
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temperature: 0.01 | top_p: 0.01]\n",
      "Softly gentle eyes\n",
      "Llama's gentle, fuzzy form\n",
      "Misty mountain home\n",
      "\n",
      "[temperature: 0.01 | top_p: 0.01]\n",
      "Softly gentle eyes\n",
      "Llama's gentle, fuzzy form\n",
      "Misty mountain home\n",
      "\n",
      "[temperature: 1.0 | top_p: 1.0]\n",
      "Softly gentle eyes\n",
      "Llama's quiet gentle step\n",
      "And a gentle heart\n",
      "\n",
      "[temperature: 1.0 | top_p: 1.0]\n",
      "Fuzzy gentle friend\n",
      "Softly padding through the night\n",
      "Llama's gentle eyes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_tuned_completion(temperature: float, top_p: float):\n",
    "    response = completion(\"Write a haiku about llamas\", temperature=temperature, top_p=top_p)\n",
    "    print(f'[temperature: {temperature} | top_p: {top_p}]\\n{response.strip()}\\n')\n",
    "\n",
    "print_tuned_completion(0.01, 0.01)\n",
    "print_tuned_completion(0.01, 0.01)\n",
    "# These two generations are highly likely to be the same\n",
    "\n",
    "print_tuned_completion(1.0, 1.0)\n",
    "print_tuned_completion(1.0, 1.0)\n",
    "# These two generations are highly likely to be different"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit Instructions\n",
    "\n",
    "Detailed, explicit instructions produce better results than open-ended prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Describe quantum physics in one short sentence of no more than 12 words\n",
      "==============\n",
      "Study of matter and energy at an atomic and subatomic level.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(prompt=\"Describe quantum physics in one short sentence of no more than 12 words\")\n",
    "# Returns a succinct explanation of quantum physics that mentions particles and states existing simultaneously."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think about giving explicit instructions as using rules and restrictions to how Llama 3 responds to your prompt.\n",
    "\n",
    "- Stylization\n",
    "    - `Explain this to me like a topic on a children's educational network show teaching elementary students.`\n",
    "    - `I'm a software engineer using large language models for summarization. Summarize the following text in under 250 words:`\n",
    "    - `Give your answer like an old timey private investigator hunting down a case step by step.`\n",
    "- Formatting\n",
    "    - `Use bullet points.`\n",
    "    - `Return as a JSON object.`\n",
    "    - `Use less technical terms and help me apply it in my work in communications.`\n",
    "- Restrictions\n",
    "    - `Only use academic papers.`\n",
    "    - `Never give sources older than 2020.`\n",
    "    - `If you don't know the answer, say that you don't know.`\n",
    "\n",
    "Here's an example of giving explicit instructions to give more specific results by limiting the responses to recently created sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Explain the latest advances in large language models to me.\n",
      "==============\n",
      "Large language models (LLMs) have made significant progress in recent years, with several key advancements. Here are some of the latest developments:\n",
      "\n",
      "1. **Transformer Architectures**: The transformer architecture, introduced in 2017, has become the foundation for many state-of-the-art LLMs. This architecture uses self-attention mechanisms to process input sequences in parallel, allowing for faster training and more efficient processing of long-range dependencies.\n",
      "\n",
      "2. **Scaling Up Models**: Researchers have been scaling up LLMs to unprecedented sizes, with models like OpenAI's GPT-3 (175 billion parameters) and Google's PaLM (540 billion parameters). These massive models have achieved impressive results on various natural language processing (NLP) tasks.\n",
      "\n",
      "3. **Multitask Learning**: Multitask learning has become a popular approach for training LLMs. This involves training a single model on multiple tasks simultaneously, allowing the model to learn a more general representation of language. This approach has been shown to improve performance on individual tasks and enable the model to adapt to new tasks more easily.\n",
      "\n",
      "4. **Pre-Training and Fine-Tuning**: The pre-training and fine-tuning paradigm has become the standard approach for training LLMs. This involves pre-training a model on a large corpus of text data, followed by fine-tuning on specific tasks. This approach allows the model to learn a general representation of language and then adapt to specific tasks.\n",
      "\n",
      "5. **Efficient Training Methods**: Researchers have developed new efficient training methods, such as knowledge distillation, to reduce the computational resources required for training LLMs. These methods involve training a smaller model (the student) to mimic the behavior of a larger model (the teacher).\n",
      "\n",
      "6. **Explainability and Interpretability**: There is a growing interest in developing techniques to explain and interpret the decisions made by LLMs. This includes techniques like attention visualization, feature importance, and model-agnostic interpretability methods.\n",
      "\n",
      "7. **Specialized Models**: Researchers have been developing specialized models for specific tasks, such as conversational AI, text generation, and language translation. These models are designed to excel in specific areas and often outperform more general-purpose LLMs.\n",
      "\n",
      "8. **Multilingual and Multimodal Models**: There is a growing interest in developing models that can handle multiple languages and modalities (e.g., text, images, audio). These models have the potential to enable more inclusive and accessible AI applications.\n",
      "\n",
      "9. **Adversarial Training**: Researchers have been exploring adversarial training methods to improve the robustness of LLMs. This involves training the model to be more resilient to adversarial attacks, which can help prevent the model from being exploited by malicious actors.\n",
      "\n",
      "10. **Quantization and Pruning**: Researchers have been developing techniques to reduce the computational resources required for deploying LLMs. This includes quantization (reducing the precision of model weights) and pruning (removing redundant weights).\n",
      "\n",
      "These advances have significantly improved the performance and efficiency of LLMs, enabling a wide range of applications, from conversational AI to language translation and text generation.\n",
      "\n",
      "==============\n",
      "Explain the latest advances in large language models to me. Always cite your sources. Never cite sources older than 2020.\n",
      "==============\n",
      "Recent advances in large language models have led to significant improvements in their performance and capabilities. Some of the key developments include:\n",
      "\n",
      "1. **Transformer-XL**: This model, introduced in a 2020 paper by the Google Brain team, uses a novel attention mechanism and a recurrence mechanism to improve the performance of transformers on long-range dependencies (Dai et al., 2020). This has led to state-of-the-art results on several natural language processing (NLP) benchmarks.\n",
      "\n",
      "2. **Switch Transformers**: In a 2021 paper, the Google team introduced the Switch Transformer, which uses a novel routing mechanism to scale up transformers to larger sizes (Fedus et al., 2021). This allows for more efficient use of computational resources and has led to significant improvements in performance on several NLP tasks.\n",
      "\n",
      "3. **Multitask Learning**: A 2022 paper by the Meta AI team demonstrated the effectiveness of multitask learning for large language models (Aghajanyan et al., 2022). By training a single model on multiple tasks simultaneously, they achieved state-of-the-art results on several NLP benchmarks.\n",
      "\n",
      "4. **Efficient Training Methods**: Researchers have also made significant progress in developing more efficient training methods for large language models. For example, a 2022 paper by the Microsoft team introduced a new training method called \"Knowledge Distillation\" that allows for more efficient training of large language models (Hinton et al., 2022).\n",
      "\n",
      "5. **Explainability and Interpretability**: There has also been significant progress in developing methods for explaining and interpreting the decisions made by large language models. For example, a 2022 paper by the Google team introduced a new method for visualizing the attention patterns of transformers (Abnar et al., 2022).\n",
      "\n",
      "References:\n",
      "Abnar, S., et al. (2022). Quantifying attention flow in transformers. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics, 2022, 1-13.\n",
      "\n",
      "Aghajanyan, A., et al. (2022). CPM: A Large-scale Generative Chinese Pre-trained Language Model. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics, 2022, 1-12.\n",
      "\n",
      "Dai, Z., et al. (2020). Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, 1-12.\n",
      "\n",
      "Fedus, W., et al. (2021). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. Proceedings of the 2021 International Conference on Learning Representations, 2021, 1-13.\n",
      "\n",
      "Hinton, G. E., et al. (2022). Distilling the Knowledge in a Neural Network. arXiv preprint arXiv:2201.10045, 2022.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"Explain the latest advances in large language models to me.\")\n",
    "# More likely to cite sources from 2017\n",
    "\n",
    "complete_and_print(\"Explain the latest advances in large language models to me. Always cite your sources. Never cite sources older than 2020.\")\n",
    "# Gives more specific advances and only cites sources from 2020"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Prompting using Zero- and Few-Shot Learning\n",
    "\n",
    "A shot is an example or demonstration of what type of prompt and response you expect from a large language model. This term originates from training computer vision models on photographs, where one shot was one example or instance that the model used to classify an image ([Fei-Fei et al. (2006)](http://vision.stanford.edu/documents/Fei-FeiFergusPerona2006.pdf)).\n",
    "\n",
    "#### Zero-Shot Prompting\n",
    "\n",
    "Large language models like Llama 3 are unique because they are capable of following instructions and producing responses without having previously seen an example of a task. Prompting without examples is called \"zero-shot prompting\".\n",
    "\n",
    "Let's try using Llama 3 as a sentiment detector. You may notice that output format varies - we can improve this with better prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Text: This was the best movie I've ever seen! \n",
      " The sentiment of the text is: \n",
      "==============\n",
      "The sentiment of the text is: Positive\n",
      "\n",
      "==============\n",
      "Text: The director was trying too hard. \n",
      " The sentiment of the text is: \n",
      "==============\n",
      "The sentiment of the text is: Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"Text: This was the best movie I've ever seen! \\n The sentiment of the text is: \")\n",
    "# Returns positive sentiment\n",
    "\n",
    "complete_and_print(\"Text: The director was trying too hard. \\n The sentiment of the text is: \")\n",
    "# Returns negative sentiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Few-Shot Prompting\n",
    "\n",
    "Adding specific examples of your desired output generally results in more accurate, consistent output. This technique is called \"few-shot prompting\".\n",
    "\n",
    "In this example, the generated response follows our desired format that offers a more nuanced sentiment classifer that gives a positive, neutral, and negative response confidence percentage.\n",
    "\n",
    "See also: [Zhao et al. (2021)](https://arxiv.org/abs/2102.09690), [Liu et al. (2021)](https://arxiv.org/abs/2101.06804), [Su et al. (2022)](https://arxiv.org/abs/2209.01975), [Rubin et al. (2022)](https://arxiv.org/abs/2112.08633).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: I thought it was okay\n",
      "20% positive 60% neutral 20% negative\n",
      "INPUT: I loved it!\n",
      "95% positive 5% neutral 0% negative\n",
      "INPUT: Terrible service 0/10\n",
      "0% positive 0% neutral 100% negative\n"
     ]
    }
   ],
   "source": [
    "def sentiment(text):\n",
    "    response = chat_completion(messages=[\n",
    "        user(\"You are a sentiment classifier. For each message, give the percentage of positive/netural/negative.\"),\n",
    "        user(\"I liked it\"),\n",
    "        assistant(\"70% positive 30% neutral 0% negative\"),\n",
    "        user(\"It could be better\"),\n",
    "        assistant(\"0% positive 50% neutral 50% negative\"),\n",
    "        user(\"It's fine\"),\n",
    "        assistant(\"25% positive 50% neutral 25% negative\"),\n",
    "        user(text),\n",
    "    ])\n",
    "    return response\n",
    "\n",
    "def print_sentiment(text):\n",
    "    print(f'INPUT: {text}')\n",
    "    print(sentiment(text))\n",
    "\n",
    "print_sentiment(\"I thought it was okay\")\n",
    "# More likely to return a balanced mix of positive, neutral, and negative\n",
    "print_sentiment(\"I loved it!\")\n",
    "# More likely to return 100% positive\n",
    "print_sentiment(\"Terrible service 0/10\")\n",
    "# More likely to return 100% negative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role Prompting\n",
    "\n",
    "Llama will often give more consistent responses when given a role ([Kong et al. (2023)](https://browse.arxiv.org/pdf/2308.07702.pdf)). Roles give context to the LLM on what type of answers are desired.\n",
    "\n",
    "Let's use Llama 3 to create a more focused, technical response for a question around the pros and cons of using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Explain the pros and cons of using PyTorch.\n",
      "==============\n",
      "**PyTorch Pros and Cons**\n",
      "==========================\n",
      "\n",
      "PyTorch is a popular open-source machine learning library developed by Facebook's AI Research Lab (FAIR). It provides a dynamic computation graph and automatic differentiation for rapid prototyping and research. Here are the pros and cons of using PyTorch:\n",
      "\n",
      "**Pros**\n",
      "--------\n",
      "\n",
      "### 1. **Dynamic Computation Graph**\n",
      "\n",
      "PyTorch's dynamic computation graph allows for more flexibility and ease of use compared to static computation graphs used in TensorFlow. This makes it ideal for rapid prototyping and research.\n",
      "\n",
      "### 2. **Automatic Differentiation**\n",
      "\n",
      "PyTorch's automatic differentiation feature simplifies the process of computing gradients, making it easier to implement complex neural networks.\n",
      "\n",
      "### 3. **Rapid Prototyping**\n",
      "\n",
      "PyTorch's Pythonic API and dynamic computation graph make it ideal for rapid prototyping and research. It allows for quick experimentation and testing of new ideas.\n",
      "\n",
      "### 4. **Large Community**\n",
      "\n",
      "PyTorch has a large and active community of developers and researchers, which means there are many pre-built models and tools available for use.\n",
      "\n",
      "### 5. **Extensive Documentation**\n",
      "\n",
      "PyTorch has extensive documentation and tutorials, making it easier for new users to get started.\n",
      "\n",
      "### 6. **Support for Distributed Training**\n",
      "\n",
      "PyTorch provides built-in support for distributed training, making it easier to scale up training to large datasets and complex models.\n",
      "\n",
      "### 7. **Native Support for CUDA**\n",
      "\n",
      "PyTorch has native support for CUDA, making it easy to run models on NVIDIA GPUs.\n",
      "\n",
      "**Cons**\n",
      "--------\n",
      "\n",
      "### 1. **Less Mature Than TensorFlow**\n",
      "\n",
      "PyTorch is a relatively new library compared to TensorFlow, which means it may not have the same level of maturity and stability.\n",
      "\n",
      "### 2. **Limited Support for Production Environments**\n",
      "\n",
      "PyTorch is primarily designed for research and rapid prototyping, which means it may not have the same level of support for production environments as TensorFlow.\n",
      "\n",
      "### 3. **Less Extensive Model Zoo**\n",
      "\n",
      "PyTorch's model zoo is not as extensive as TensorFlow's, which means there may be fewer pre-built models available for use.\n",
      "\n",
      "### 4. **Less Support for Mobile and Embedded Devices**\n",
      "\n",
      "PyTorch has limited support for mobile and embedded devices, making it less suitable for applications that require deployment on these platforms.\n",
      "\n",
      "### 5. **Steeper Learning Curve for Complex Models**\n",
      "\n",
      "While PyTorch is generally easy to use, it can have a steeper learning curve for complex models and distributed training.\n",
      "\n",
      "**Example Use Case**\n",
      "--------------------\n",
      "\n",
      "Here's an example of using PyTorch to build a simple neural network:\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "\n",
      "# Define the model\n",
      "class Net(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Net, self).__init__()\n",
      "        self.fc1 = nn.Linear(784, 128)\n",
      "        self.fc2 = nn.Linear(128, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = torch.relu(self.fc1(x))\n",
      "        x = self.fc2(x)\n",
      "        return x\n",
      "\n",
      "# Initialize the model, loss function, and optimizer\n",
      "model = Net()\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
      "\n",
      "# Train the model\n",
      "for epoch in range(10):\n",
      "    # Forward pass\n",
      "    outputs = model(inputs)\n",
      "    loss = criterion(outputs, labels)\n",
      "\n",
      "    # Backward pass\n",
      "    optimizer.zero_grad()\n",
      "    loss.backward()\n",
      "    optimizer.step()\n",
      "```\n",
      "This example demonstrates how to define a simple neural network using PyTorch's `nn.Module` API and train it using the `SGD` optimizer.\n",
      "\n",
      "==============\n",
      "Your role is a machine learning expert who gives highly technical advice to senior engineers who work with complicated datasets. Explain the pros and cons of using PyTorch.\n",
      "==============\n",
      "**PyTorch: A Technical Overview for Senior Engineers**\n",
      "\n",
      "As a machine learning expert, I'll provide a detailed analysis of PyTorch, a popular open-source machine learning framework. This overview will cover the pros and cons of using PyTorch, focusing on its technical aspects.\n",
      "\n",
      "**Pros:**\n",
      "\n",
      "### 1. **Dynamic Computation Graph**\n",
      "\n",
      "PyTorch's dynamic computation graph (DCG) allows for more flexibility and ease of use compared to static computation graphs (SCGs) used in TensorFlow. The DCG enables on-the-fly graph construction, making it easier to implement complex models and debug issues.\n",
      "\n",
      "### 2. **Autograd System**\n",
      "\n",
      "PyTorch's autograd system provides automatic differentiation, which simplifies the process of computing gradients. This feature is particularly useful for optimizing complex models and debugging gradient-related issues.\n",
      "\n",
      "### 3. **Modular Architecture**\n",
      "\n",
      "PyTorch's modular architecture allows for easy extension and customization of the framework. This modularity enables developers to create custom modules and integrate them seamlessly with the existing framework.\n",
      "\n",
      "### 4. **Rapid Prototyping**\n",
      "\n",
      "PyTorch's Pythonic API and dynamic computation graph make it an ideal choice for rapid prototyping and research. The framework's flexibility and ease of use enable developers to quickly test and iterate on new ideas.\n",
      "\n",
      "### 5. **Extensive Community Support**\n",
      "\n",
      "PyTorch has a large and active community, which contributes to its extensive documentation, tutorials, and pre-built models. This community support makes it easier for developers to find resources and resolve issues.\n",
      "\n",
      "**Cons:**\n",
      "\n",
      "### 1. **Performance Overhead**\n",
      "\n",
      "PyTorch's dynamic computation graph and autograd system introduce a performance overhead compared to static computation graphs. This overhead can result in slower training times for large models.\n",
      "\n",
      "### 2. **Limited Support for Distributed Training**\n",
      "\n",
      "While PyTorch provides some support for distributed training, it is not as extensive as TensorFlow's support. This limited support can make it more challenging to scale PyTorch models to large clusters.\n",
      "\n",
      "### 3. **Less Mature than TensorFlow**\n",
      "\n",
      "PyTorch is a relatively newer framework compared to TensorFlow, which means it may not have the same level of maturity and stability. This can result in more frequent updates and breaking changes.\n",
      "\n",
      "### 4. **Limited Support for Production Deployment**\n",
      "\n",
      "PyTorch's focus on research and prototyping means it may not have the same level of support for production deployment as TensorFlow. This can make it more challenging to deploy PyTorch models in production environments.\n",
      "\n",
      "### 5. **GPU Memory Management**\n",
      "\n",
      "PyTorch's dynamic computation graph can lead to GPU memory management issues, particularly when working with large models. This can result in slower training times and increased memory usage.\n",
      "\n",
      "**Best Practices for Using PyTorch:**\n",
      "\n",
      "1. **Use PyTorch for rapid prototyping and research**: PyTorch's flexibility and ease of use make it an ideal choice for quickly testing and iterating on new ideas.\n",
      "2. **Optimize PyTorch models for performance**: Use techniques like model pruning, knowledge distillation, and mixed precision training to optimize PyTorch models for performance.\n",
      "3. **Use PyTorch's built-in support for distributed training**: While PyTorch's support for distributed training is limited, it can still be used to scale models to smaller clusters.\n",
      "4. **Monitor GPU memory usage**: Regularly monitor GPU memory usage to avoid memory management issues and optimize training times.\n",
      "\n",
      "In conclusion, PyTorch is a powerful and flexible machine learning framework that excels in rapid prototyping and research. While it may have some performance overhead and limited support for distributed training, it can still be used to build and deploy complex models. By following best practices and optimizing PyTorch models for performance, developers can unlock the full potential of this framework.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"Explain the pros and cons of using PyTorch.\")\n",
    "# More likely to explain the pros and cons of PyTorch covers general areas like documentation, the PyTorch community, and mentions a steep learning curve\n",
    "\n",
    "complete_and_print(\"Your role is a machine learning expert who gives highly technical advice to senior engineers who work with complicated datasets. Explain the pros and cons of using PyTorch.\")\n",
    "# Often results in more technical benefits and drawbacks that provide more technical details on how model layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-Thought\n",
    "\n",
    "Simply adding a phrase encouraging step-by-step thinking \"significantly improves the ability of large language models to perform complex reasoning\" ([Wei et al. (2022)](https://arxiv.org/abs/2201.11903)). This technique is called \"CoT\" or \"Chain-of-Thought\" prompting.\n",
    "\n",
    "Llama 3.1 now reasons step-by-step naturally without the addition of the phrase. This section remains for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Who lived longer, Mozart or Elvis?\n",
      "==============\n",
      "Wolfgang Amadeus Mozart (1756-1791) lived to be 35 years old. \n",
      "\n",
      "Elvis Presley (1935-1977) lived to be 42 years old.\n",
      "\n",
      "So, Elvis Presley lived about 7 years longer than Mozart.\n",
      "\n",
      "==============\n",
      "Who lived longer, Mozart or Elvis? Let's think through this carefully, step by step.\n",
      "==============\n",
      "To determine who lived longer, Mozart or Elvis, let's analyze their lifespans step by step.\n",
      "\n",
      "1. First, we need to know their birth and death dates:\n",
      "   - Wolfgang Amadeus Mozart was born on January 27, 1756.\n",
      "   - Elvis Presley was born on January 8, 1935.\n",
      "   - Mozart passed away on December 5, 1791.\n",
      "   - Elvis Presley passed away on August 16, 1977.\n",
      "\n",
      "2. Now, let's calculate their lifespans:\n",
      "   - Mozart lived from January 27, 1756, to December 5, 1791, which is approximately 35 years.\n",
      "   - Elvis Presley lived from January 8, 1935, to August 16, 1977, which is approximately 42 years.\n",
      "\n",
      "Based on these calculations, Elvis Presley lived about 7 years longer than Wolfgang Amadeus Mozart.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who lived longer, Mozart or Elvis?\"\n",
    "\n",
    "complete_and_print(prompt)\n",
    "# Llama 2 would often give the incorrect answer of \"Mozart\"\n",
    "\n",
    "complete_and_print(f\"{prompt} Let's think through this carefully, step by step.\")\n",
    "# Gives the correct answer \"Elvis\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Consistency\n",
    "\n",
    "LLMs are probablistic, so even with Chain-of-Thought, a single generation might produce incorrect results. Self-Consistency ([Wang et al. (2022)](https://arxiv.org/abs/2203.11171)) introduces enhanced accuracy by selecting the most frequent answer from multiple generations (at the cost of higher compute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers: ['50', '50', '50', '50', '50']\n",
      " Final answer: 50\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from statistics import mode\n",
    "\n",
    "def gen_answer():\n",
    "    response = completion(\n",
    "        \"John found that the average of 15 numbers is 40.\"\n",
    "        \"If 10 is added to each number then the mean of the numbers is?\"\n",
    "        \"Report the answer surrounded by backticks (example: `123`)\",\n",
    "    )\n",
    "    match = re.search(r'`(\\d+)`', response)\n",
    "    if match is None:\n",
    "        return None\n",
    "    return match.group(1)\n",
    "\n",
    "answers = [gen_answer() for i in range(5)]\n",
    "\n",
    "print(\n",
    "    f\"Answers: {answers}\\n\",\n",
    "    f\"Final answer: {mode(answers)}\",\n",
    "    )\n",
    "\n",
    "# Sample runs of Llama-3-70B (all correct):\n",
    "# ['60', '50', '50', '50', '50'] -> 50\n",
    "# ['50', '50', '50', '60', '50'] -> 50\n",
    "# ['50', '50', '60', '50', '50'] -> 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation\n",
    "\n",
    "You'll probably want to use factual knowledge in your application. You can extract common facts from today's large models out-of-the-box (i.e. using just the model weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "What is the capital of the Thailand?\n",
      "==============\n",
      "The capital of Thailand is Bangkok.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"What is the capital of the Thailand?\")\n",
    "# Gives the correct answer \"Sacramento\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, more specific facts, or private information, cannot be reliably retrieved. The model will either declare it does not know or hallucinate an incorrect answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "What was the temperature in Menlo Park on December 12th, 2023?\n",
      "==============\n",
      "I'm not able to provide real-time or recent weather information, including specific temperatures for certain dates and locations. However, I can suggest some options for you to find the temperature in Menlo Park on December 12th, 2023:\n",
      "\n",
      "1. Check online weather websites: You can visit websites like AccuWeather, Weather.com, or the National Weather Service (NWS) to see if they have archived weather data for Menlo Park on December 12th, 2023.\n",
      "2. Contact local weather stations: Reach out to local weather stations or news outlets in the Menlo Park area to see if they have any records of the temperature on December 12th, 2023.\n",
      "3. Check social media: Look for social media posts from local residents, weather enthusiasts, or weather-related accounts that may have shared information about the temperature on December 12th, 2023.\n",
      "\n",
      "I hope this helps, and I apologize for not being able to provide the specific information you were looking for.\n",
      "\n",
      "==============\n",
      "What time is my dinner reservation on Saturday and what should I wear?\n",
      "==============\n",
      "I'm happy to help, but I don't have any information about your dinner reservation or personal details. I'm a large language model, I don't have the ability to access or store information about your personal life or schedule. If you need to confirm your dinner reservation or dress code, I recommend checking your calendar or contacting the restaurant directly.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"What was the temperature in Menlo Park on December 12th, 2023?\")\n",
    "# \"I'm just an AI, I don't have access to real-time weather data or historical weather records.\"\n",
    "\n",
    "complete_and_print(\"What time is my dinner reservation on Saturday and what should I wear?\")\n",
    "# \"I'm not able to access your personal information [..] I can provide some general guidance\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation, or RAG, describes the practice of including information in the prompt you've retrived from an external database ([Lewis et al. (2020)](https://arxiv.org/abs/2005.11401v4)). It's an effective way to incorporate facts into your LLM application and is more affordable than fine-tuning which may be costly and negatively impact the foundational model's capabilities.\n",
    "\n",
    "This could be as simple as a lookup table or as sophisticated as a [vector database]([FAISS](https://github.com/facebookresearch/faiss)) containing all of your company's knowledge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Given the following information: 'The temperature in Menlo Park was 51 degrees Fahrenheit on 2023-12-12'', respond to: 'What is the temperature in Menlo Park on 2023-12-12?'\n",
      "==============\n",
      "The temperature in Menlo Park on 2023-12-12 was 51 degrees Fahrenheit.\n",
      "\n",
      "==============\n",
      "Given the following information: 'The temperature in Menlo Park was unknown temperature on 2023-07-18'', respond to: 'What is the temperature in Menlo Park on 2023-07-18?'\n",
      "==============\n",
      "The temperature in Menlo Park on 2023-07-18 is unknown.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MENLO_PARK_TEMPS = {\n",
    "    \"2023-12-11\": \"52 degrees Fahrenheit\",\n",
    "    \"2023-12-12\": \"51 degrees Fahrenheit\",\n",
    "    \"2023-12-13\": \"51 degrees Fahrenheit\",\n",
    "}\n",
    "\n",
    "\n",
    "def prompt_with_rag(retrived_info, question):\n",
    "    complete_and_print(\n",
    "        f\"Given the following information: '{retrived_info}', respond to: '{question}'\"\n",
    "    )\n",
    "\n",
    "\n",
    "def ask_for_temperature(day):\n",
    "    temp_on_day = MENLO_PARK_TEMPS.get(day) or \"unknown temperature\"\n",
    "    prompt_with_rag(\n",
    "        f\"The temperature in Menlo Park was {temp_on_day} on {day}'\",  # Retrieved fact\n",
    "        f\"What is the temperature in Menlo Park on {day}?\",  # User question\n",
    "    )\n",
    "\n",
    "\n",
    "ask_for_temperature(\"2023-12-12\")\n",
    "# \"Sure! The temperature in Menlo Park on 2023-12-12 was 51 degrees Fahrenheit.\"\n",
    "\n",
    "ask_for_temperature(\"2023-07-18\")\n",
    "# \"I'm not able to provide the temperature in Menlo Park on 2023-07-18 as the information provided states that the temperature was unknown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "เมืองหลวงของประเทศไทยคืออะไร?\n",
      "==============\n",
      "กรุงเทพมหานคร\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"เมืองหลวงของประเทศไทยคืออะไร?\")\n",
    "# Gives the correct answer \"Sacramento\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_3\n",
      "==============\n",
      "Assistant --> บทบาทของคุณคือนักแก้คำผิด จงแก้คำผิดของ : 'ขันตอนการผลิตและควบคุณภาพ' โดยแสดงเฉพาะคำตอบ\n",
      "==============\n",
      "user --> ขั้นตอนการผลิตและควบคุมคุณภาพ\n",
      "\n",
      "page_4\n",
      "==============\n",
      "Assistant --> บทบาทของคุณคือนักแก้คำผิด จงแก้คำผิดของ : 'การตรวจจสฝู้อบกอนการเทคอนกรต\n",
      "®การเตรียมโมล\n",
      "1,ตรวจสอบชนิด(type)ของโมลให้ตรงกับแบบและทําความสะอาดโมลโดยใช้ลมเป่า\n",
      "2,ใช้นํายาทาแบบทาบริเวณขอบรอยต่อของโมลก่อนการปิดโมล\n",
      "3,เน้นการทาน้ามันบริเวณขอบยาง(แย5๐0เพื่อป้องกันไม่ให้ยางเกิดความเสียหาย\n",
      "ขณะเปิด-ปิด' โดยแสดงเฉพาะคำตอบ\n",
      "==============\n",
      "user --> การตรวจสอบฝุ่นผง่อนก่อนการเทคอนกรีต\n",
      "การเตรียมโมลด์\n",
      "1. ตรวจสอบชนิด (type) ของโมลด์ให้ตรงกับแบบและทำความสะอาดโมลด์โดยใช้ลมเป่า\n",
      "2. ใช้น้ำยาทาแบบทาบริเวณขอบรอยต่อของโมลด์ก่อนการปิดโมลด์\n",
      "3. เน้นการทาน้ำมันบริเวณขอบยาง (O-ring 500) เพื่อป้องกันไม่ให้ยางเกิดความเสียหายขณะเปิด-ปิด\n",
      "\n",
      "page_5\n",
      "==============\n",
      "Assistant --> บทบาทของคุณคือนักแก้คำผิด จงแก้คำผิดของ : '4,หลังจากปิดด้านข้างทาทั้ง4ด้านและท้องแบบ\n",
      "5,ทานํายาทาแบบบริเวณฝาปิดทั้งสองข้าง\n",
      "NOTE:นํายาทาแบบทิใช้ต้องผ่านการอนุมัติโดยเจ้าของงาน/ตัวแทนเจ้าของงาน\n",
      "6,ทาจารบีที่POCKETเพื่อป้องกันไม่ให้เกิดความเสียหายโดยทาให้ครบ\n",
      "ตามจํานวนของแต่ละโมล' โดยแสดงเฉพาะคำตอบ\n",
      "==============\n",
      "user --> 1. หลังจากปิดด้านข้าง ทาทั้ง 4 ด้านและท้องแบบ\n",
      "2. ทาน้ำยาแบบบริเวณฝาปิดทั้งสองข้าง\n",
      "3. น้ำยาทาแบบที่ใช้ต้องผ่านการอนุมัติโดยเจ้าของงาน/ตัวแทนเจ้าของงาน\n",
      "4. ทาจารบีที่ POCKET เพื่อป้องกันไม่ให้เกิดความเสียหาย โดยทาให้ครบตามจำนวนของแต่ละโมดูล\n",
      "\n",
      "page_6\n",
      "==============\n",
      "Assistant --> บทบาทของคุณคือนักแก้คำผิด จงแก้คำผิดของ : '7,นํา000657ใส่กลับเข้าไปยังตําแหน่งเดิมให้ครบตามจํานวจนของแต่ละโมล\n",
      "8,ตรวจสอบเหล็กที่ผูกเสร็จแล้วที่ง|ยโดยมีวิธีตรวจสอบเหล็กเสริมคอนกรีตดังนี่\n",
      "-จํานวนของเหล็กเสริม(ตามทีระบุไว้ในแบบ)\n",
      "-ขนาดของเหล็กเสริม(ตามที่ระบุไว้ในแบบ)\n",
      "-ระยะห่างของเหล็กเสริม(ตามที่ระบุไว้ในแบบ)\n",
      "-ตําแหน่งของเหล็กเสริม(ตามที่ระบุไว้ในแบบ)\n",
      "-ตําแหน่งลูกปูน(Covering)\n",
      "-สภาพผิวของเหล็กเสริม(สนิม,อื่นๆ)\n",
      "-การเก็บงานของปลายลวดผูกเหล็ก' โดยแสดงเฉพาะคำตอบ\n",
      "==============\n",
      "user --> 7. นำ 7,000.657 ใส่กลับเข้าไปยังตำแหน่งเดิมให้ครบตามจำนวนของแต่ละโมล\n",
      "\n",
      "8. ตรวจสอบเหล็กที่ผูกเสร็จแล้วที่ง่าย โดยมีวิธีตรวจสอบเหล็กเสริมคอนกรีตดังนี้\n",
      "- จำนวนของเหล็กเสริม (ตามที่ระบุไว้ในแบบ)\n",
      "- ขนาดของเหล็กเสริม (ตามที่ระบุไว้ในแบบ)\n",
      "- ระยะห่างของเหล็กเสริม (ตามที่ระบุไว้ในแบบ)\n",
      "- ตำแหน่งของเหล็กเสริม (ตามที่ระบุไว้ในแบบ)\n",
      "- ตำแหน่งลูกปูน (Covering)\n",
      "- สภาพผิวของเหล็กเสริม (สนิม, อื่นๆ)\n",
      "- การเก็บงานของปลายลวดผูกเหล็ก\n",
      "\n",
      "page_7\n",
      "==============\n",
      "Assistant --> บทบาทของคุณคือนักแก้คำผิด จงแก้คำผิดของ : '9,วางเหล็กที่ตรวจสอบแล้วลงในโมล\n",
      "10,จัดเหล็กให้พอดีกับช่องจ่างของInsertในโมลและตรวสอบตําแหน่งของลูกปูน\n",
      "11,ทาจารบีให้ทั่วธแล|ฮูท!Boltสีเขียวทัง2ตัวให้ครบตามจํานวนของแต่ละโมล' โดยแสดงเฉพาะคำตอบ\n",
      "==============\n",
      "user --> 9. วางเหล็กที่ตรวจสอบแล้วลงในโมลด์\n",
      "10. จัดเหล็กให้พอดีกับช่องว่างของอินเสิร์ตในโมลด์และตรวจสอบตำแหน่งของลูกปูน\n",
      "11. ทาจารบีให้ทั่วทั้ง 2 ตัวบोलต์สีเขียวให้ครบตามจำนวนของแต่ละโมลด์\n",
      "\n",
      "page_8\n",
      "==============\n",
      "Assistant --> บทบาทของคุณคือนักแก้คำผิด จงแก้คำผิดของ : '12,นํากลับไปใส่ให้แน่นตามตําแหน่งในฝั่งด้านสั้น\n",
      "13,ทาจารบีที่โธเห๒๐๒จCURVEDBOLTให้ครบตามจํานวนในแต่ละโมล\n",
      "44,เตรียมCURVEDPLASTICSHEATHให้ครบตามจํานวนในแต่ละโมล' โดยแสดงเฉพาะคำตอบ\n",
      "==============\n",
      "user --> 1. นำ\n",
      "2. ตำแหน่ง\n",
      "3. โธ\n",
      "4. จารบี\n",
      "5. โมล (ไม่มีคำผิด)\n",
      "6. โธ\n",
      "7. จํานวน (ไม่มีคำผิด)\n",
      "8. โธ\n",
      "9. จํานวน (ไม่มีคำผิด)\n",
      "\n",
      "คำตอบที่แก้ไขแล้ว:\n",
      "'12,นำกลับไปใส่ให้แน่นตามตำแหน่งในฝั่งด้านสั้น\n",
      "13,ทาจารบีที่โธเห๒๐๒จCURVEDBOLTให้ครบตามจำนวนในแต่ละโมล\n",
      "44,เตรียมCURVEDPLASTICSHEATHให้ครบตามจำนวนในแต่ละโมล'\n",
      "\n",
      "page_9\n",
      "==============\n",
      "Assistant --> บทบาทของคุณคือนักแก้คำผิด จงแก้คำผิดของ : '45,นําTEMPORARYCURVEDBOLTกลับเข้าไปฝังในโมลพร้อมกับสวม|\n",
      "CURVEDPLASTICSHEATHแล้วทําการยืดให้แน่นป\n",
      "46,นําPLASTICSOCKETสวมเกลียงพร้อมขันล็อคให้แน่นสุดเกลียวจนปาก\n",
      "PLASTICSOCKETแนบสนิทกับโมล\n",
      "17,ใช้SocketWrenchขันSOCKETHOLEตามทิศทางให้แน่นเพื่อป้องกัน\n",
      "ไม่ให้น้้าปูนไหลเข้าไปด้านใน' โดยแสดงเฉพาะคำตอบ\n",
      "==============\n",
      "user --> 45. นำTEMPORARY CURVED BOLTกลับเข้าไปฝังในโมลพร้อมกับสวม CURVED PLASTIC SHEATH แล้วทำการยืดให้แน่นป\n",
      "\n",
      "46. นำPLASTIC SOCKETสวมเกลียวพร้อมขันล็อคให้แน่นสุดเกลียวจนปาก PLASTIC SOCKETแนบสนิทกับโมล\n",
      "\n",
      "17. ใช้Socket WrenchขันSOCKET HOLEตามทิศทางให้แน่นเพื่อป้องกันไม่ให้น้ำปูนไหลเข้าไปด้านใน\n",
      "\n",
      "page_10\n",
      "==============\n",
      "Assistant --> บทบาทของคุณคือนักแก้คำผิด จงแก้คำผิดของ : '48,นําGROUTPLUGมาปิดSOCKETHOLEเพื่อป้องน้้าปูนเข้าไปดันSOCKETHOLE\n",
      "ในขณะเทคลนกริต\n",
      "19,สุดท้ายเช็คอุปกรณ์การฝังภายในแต่ละโมลให้ครบตามจํานวนและยืดแน่นหนาพร้อมกับ\n",
      "เซ็คจํานวนของเหล็กเสริมขนาดระยะห่างตําแหน่งและระยะหุ้มของคอนกรีตให้เป็นไปตามทีระบุไว้\n",
      "ในแบบอีกคริ้งก่อนปิดฝาโมล' โดยแสดงเฉพาะคำตอบ\n",
      "==============\n",
      "user --> 48. นำ GROUT PLUG มาปิด SOCKET HOLE เพื่อป้องกันน้ำปูนเข้าไปดัน SOCKET HOLE ในขณะเทคอนกรีต\n",
      "19. สุดท้าย ตรวจสอบอุปกรณ์การฝังภายในแต่ละโมลให้ครบตามจำนวนและยืดแน่นหนาพร้อมกับ ตรวจสอบจำนวนของเหล็กเสริม ขนาด ระยะห่าง ตำแหน่ง และระยะหุ้มของคอนกรีตให้เป็นไปตามที่ระบุไว้ในแบบอีกครั้งก่อนปิดฝาโมล\n",
      "\n",
      "page_11\n",
      "==============\n",
      "Assistant --> บทบาทของคุณคือนักแก้คำผิด จงแก้คำผิดของ : '20,ปิดฝาโมลพร้อมล็อคBOLTตามจํานวนของแต่ละโมลให้แน่นเพื่อเตรียมพร้อมสําหรับ\n",
      "200งานเทคอนกรีต\n",
      "NOTE:อุปกรณ์ทิใช้การฝังทังหมดต้องผ่านการอนุมัติใช้จากเจ้าของงาน/ตัวแทนเจ้าของงาน\n",
      "ใบบันทึกการตรวจสอบ(InspectionCheckSheet)ต้องบันทึกโดยAcตามแบบฟอร์มในเอกสารแนบ' โดยแสดงเฉพาะคำตอบ\n",
      "==============\n",
      "user --> 20. ปิดฝาโมลพร้อมล็อคโบลต์ตามจำนวนของแต่ละโมลให้แน่นเพื่อเตรียมพร้อมสำหรับงานเทคอนกรีต 200\n",
      "\n",
      "หมายเหตุ: อุปกรณ์ที่ใช้การฝังทั้งหมดต้องผ่านการอนุมัติใช้จากเจ้าของงาน/ตัวแทนเจ้าของงาน\n",
      "\n",
      "ใบบันทึกการตรวจสอบ (Inspection Check Sheet) ต้องบันทึกโดย AC ตามแบบฟอร์มในเอกสารแนบ\n",
      "\n",
      "page_12\n",
      "==============\n",
      "Assistant --> บทบาทของคุณคือนักแก้คำผิด จงแก้คำผิดของ : 'ภาพถ่ายประกอบการอบรม' โดยแสดงเฉพาะคำตอบ\n",
      "==============\n",
      "user --> ภาพประกอบการอบรม\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#load the data\n",
    "with open('C:\\\\Users\\\\User\\\\llama-recipes\\\\Fibo_Project\\\\Database\\\\mold_Preparing\\\\json\\\\mold_Preparingv2.json') as f:\n",
    "    datav2 = json.load(f)\n",
    "with open('C:\\\\Users\\\\User\\\\llama-recipes\\\\Fibo_Project\\\\Database\\\\mold_Preparing\\\\json\\\\mold_Preparingv3.json') as f:\n",
    "    datav3 = json.load(f)\n",
    "\n",
    "for key in datav2.keys():\n",
    "    v2 = datav2[key].strip()\n",
    "    # print(f\"v2: {v2}\")\n",
    "    # print('compared to ---------------------------------')\n",
    "    # v3 = datav3[key].strip().split('\\n')\n",
    "    # #remove every variable space from v3\n",
    "    # v3_compared = datav3[key].split('\\n')\n",
    "    \n",
    "    # v33 = []\n",
    "    # for i in range(len(v3)):\n",
    "    #     v3_compared[i] = v3_compared[i].replace(' ', '')\n",
    "    #     #print(f'{v3_compared} == {v2} : {v3_compared[i] == v2}')\n",
    "    #     if v3_compared[i] in v2:\n",
    "    #         v33 += [v3[i]]\n",
    "        \n",
    "    # v3 = f'\\n'.join(v33)\n",
    "    # print(f\"v3: {v3}\")\n",
    "    print(f\"{key}\")\n",
    "    complete_and_print(f\"บทบาทของคุณคือนักแก้คำผิด จงแก้คำผิดของ : '{v2}' โดยแสดงเฉพาะคำตอบ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "บทบาทของคุณคือนักแก้คำผิด จงแก้คำผิดของ : 'การตรวจกอนการเทคนกรต'\n",
      "==============\n",
      "คำที่ถูกต้องควรเป็น \"การตรวจสอบก่อนการเทคนิค\" หรืออาจเป็น \"การตรวจสอบก่อนการเทคโนโลยี\" แต่คำที่ใกล้เคียงที่สุดกับคำที่ให้มา คือ \"การตรวจสอบก่อนการเทคนิค\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = 'การตรวจกอนการเทคนกรต'\n",
    "complete_and_print(f\"บทบาทของคุณคือนักแก้คำผิด จงแก้คำผิดของ : '{test}'\")\n",
    "# Often results in more technical benefits and drawbacks that provide more technical details on how model layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "เมืองหลวงของเวียดนามคือฮานอย\n"
     ]
    }
   ],
   "source": [
    "response = chat_completion(messages=[\n",
    "    user(\"เมืองหลวงของประเทศไทยคืออะไร?\"),\n",
    "    assistant(\"เมืองหลวงของประเทศไทยคือกรุงเทพมหานคร\"),\n",
    "    user(\"เมืองหลวงของเวียดนามคืออะไร?\"),\n",
    "])\n",
    "print(response)\n",
    "# \"Sure, I can help you with that! Your favorite color is blue.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Given the following information: 'The temperature in Menlo Park was 51 degrees Fahrenheit on 2023-12-12'', respond to: 'What is the temperature in Menlo Park on 2023-12-12?'\n",
      "==============\n",
      "The temperature in Menlo Park on 2023-12-12 was 51 degrees Fahrenheit.\n",
      "\n",
      "==============\n",
      "Given the following information: 'The temperature in Menlo Park was unknown temperature on 2023-07-18'', respond to: 'What is the temperature in Menlo Park on 2023-07-18?'\n",
      "==============\n",
      "The temperature in Menlo Park on 2023-07-18 is unknown.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# POCKET่กลับเข้าไปังตําแหน่งเดิมให้ครตามจํานวนของแต่ละโมลตรวจสอบเหล็กที่ผูกเสร็จแล้วที่โดยมีวิธีตรวจสอบเหล็กเสริมคนกรีตดังนี-จํานวนของเหล็กเสริม(ตามทีระบุไว้ในแบบ)-ขนาดของเหล็กเสริม(ตามที่ระบุไว้ในแบบ)-ยะ่างของเหล็กเสริม(ตามที่ระบุไว้ในแบบ)-ตําแหน่งของเหล็กเสริม(ตามที่ระบุไว้ในแบบ)-ตําแหน่งกปู่น(Covering)-สภาพผิวของเหล็กเสริม(สนิม,อื่นๆ)-การเก็บงานของปลายลวดผูกเหล็ก\n",
    "MENLO_PARK_TEMPS = {\n",
    "    \"2023-12-11\": \"52 degrees Fahrenheit\",\n",
    "    \"2023-12-12\": \"51 degrees Fahrenheit\",\n",
    "    \"2023-12-13\": \"51 degrees Fahrenheit\",\n",
    "}\n",
    "\n",
    "\n",
    "def prompt_with_rag(retrived_info, question):\n",
    "    complete_and_print(\n",
    "        f\"Given the following information: '{retrived_info}', respond to: '{question}'\"\n",
    "    )\n",
    "\n",
    "\n",
    "def ask_for_temperature(day):\n",
    "    temp_on_day = MENLO_PARK_TEMPS.get(day) or \"unknown temperature\"\n",
    "    prompt_with_rag(\n",
    "        f\"The temperature in Menlo Park was {temp_on_day} on {day}'\",  # Retrieved fact\n",
    "        f\"What is the temperature in Menlo Park on {day}?\",  # User question\n",
    "    )\n",
    "\n",
    "\n",
    "ask_for_temperature(\"2023-12-12\")\n",
    "# \"Sure! The temperature in Menlo Park on 2023-12-12 was 51 degrees Fahrenheit.\"\n",
    "\n",
    "ask_for_temperature(\"2023-07-18\")\n",
    "# \"I'm not able to provide the temperature in Menlo Park on 2023-07-18 as the information provided states that the temperature was unknown.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program-Aided Language Models\n",
    "\n",
    "LLMs, by nature, aren't great at performing calculations. Let's try:\n",
    "\n",
    "$$\n",
    "((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
    "$$\n",
    "\n",
    "(The correct answer is 91383.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "\n",
      "Calculate the answer to the following math problem:\n",
      "\n",
      "((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
      "\n",
      "==============\n",
      "To solve the problem, we'll follow the order of operations (PEMDAS):\n",
      "\n",
      "1. Evaluate the expressions inside the parentheses:\n",
      "\n",
      "   (-5 + 93 * 4 - 0)\n",
      "   = -5 + 372 - 0 (since 93 * 4 = 372)\n",
      "   = 367\n",
      "\n",
      "   (4^4 + -7 + 0 * 5)\n",
      "   = 256 + -7 + 0 (since 4^4 = 256 and 0 * 5 = 0)\n",
      "   = 249\n",
      "\n",
      "2. Now multiply the two results:\n",
      "\n",
      "   367 * 249\n",
      "   = 91483\n",
      "\n",
      "The answer to the given math problem is 91483.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"\"\"\n",
    "Calculate the answer to the following math problem:\n",
    "\n",
    "((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
    "\"\"\")\n",
    "# Gives incorrect answers like 92448, 92648, 95463"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gao et al. (2022)](https://arxiv.org/abs/2211.10435) introduced the concept of \"Program-aided Language Models\" (PAL). While LLMs are bad at arithmetic, they're great for code generation. PAL leverages this fact by instructing the LLM to write code to solve calculation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "\n",
      "    # Python code to calculate: ((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
      "    \n",
      "==============\n",
      "### Calculating the Expression\n",
      "\n",
      "Here's a Python code snippet that calculates the given expression:\n",
      "\n",
      "```python\n",
      "def calculate_expression():\n",
      "    \"\"\"\n",
      "    Calculate the expression ((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5)).\n",
      "    \n",
      "    Returns:\n",
      "    float: The result of the expression.\n",
      "    \"\"\"\n",
      "    # Calculate the first part of the expression\n",
      "    first_part = -5 + 93 * 4 - 0\n",
      "    \n",
      "    # Calculate the second part of the expression\n",
      "    # Note that in Python, the exponentiation operator is **, not ^\n",
      "    second_part = 4 ** 4 + -7 + 0 * 5\n",
      "    \n",
      "    # Multiply the two parts together\n",
      "    result = first_part * second_part\n",
      "    \n",
      "    return result\n",
      "\n",
      "# Example usage:\n",
      "result = calculate_expression()\n",
      "print(\"The result of the expression is:\", result)\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "\n",
      "This code defines a function `calculate_expression` that breaks down the given expression into two parts and calculates each part separately. The first part is `-5 + 93 * 4 - 0`, and the second part is `4^4 + -7 + 0 * 5`. Note that in Python, the exponentiation operator is `**`, not `^`.\n",
      "\n",
      "The code then multiplies the two parts together to get the final result.\n",
      "\n",
      "### Running the Code\n",
      "\n",
      "To run this code, save it to a file (e.g., `expression_calculator.py`) and execute it using Python (e.g., `python expression_calculator.py`). This will print the result of the expression to the console.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\n",
    "    \"\"\"\n",
    "    # Python code to calculate: ((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91383\n"
     ]
    }
   ],
   "source": [
    "# The following code was generated by Llama 3 70B:\n",
    "\n",
    "result = ((-5 + 93 * 4 - 0) * (4**4 - 7 + 0 * 5))\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiting Extraneous Tokens\n",
    "\n",
    "A common struggle with Llama 2 is getting output without extraneous tokens (ex. \"Sure! Here's more information on...\"), even if explicit instructions are given to Llama 2 to be concise and no preamble. Llama 3.x can better follow instructions.\n",
    "\n",
    "Check out this improvement that combines a role, rules and restrictions, explicit instructions, and an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Give me the zip code for Menlo Park in JSON format with the field 'zip_code'\n",
      "==============\n",
      "```json\n",
      "{\n",
      "  \"zip_code\": \"94025, 94026, 94027\"\n",
      "}\n",
      "```\n",
      "\n",
      "Note: Menlo Park, California has multiple zip codes: 94025, 94026, and 94027.\n",
      "\n",
      "==============\n",
      "\n",
      "    You are a robot that only outputs JSON.\n",
      "    You reply in JSON format with the field 'zip_code'.\n",
      "    Example question: What is the zip code of the Empire State Building? Example answer: {'zip_code': 10118}\n",
      "    Now here is my question: What is the zip code of Menlo Park?\n",
      "    \n",
      "==============\n",
      "{'zip_code': 94025}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\n",
    "    \"Give me the zip code for Menlo Park in JSON format with the field 'zip_code'\",\n",
    ")\n",
    "# Likely returns the JSON and also \"Sure! Here's the JSON...\"\n",
    "\n",
    "complete_and_print(\n",
    "    \"\"\"\n",
    "    You are a robot that only outputs JSON.\n",
    "    You reply in JSON format with the field 'zip_code'.\n",
    "    Example question: What is the zip code of the Empire State Building? Example answer: {'zip_code': 10118}\n",
    "    Now here is my question: What is the zip code of Menlo Park?\n",
    "    \"\"\",\n",
    ")\n",
    "# \"{'zip_code': 94025}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "- [PromptingGuide.ai](https://www.promptingguide.ai/)\n",
    "- [LearnPrompting.org](https://learnprompting.org/)\n",
    "- [Lil'Log Prompt Engineering Guide](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author & Contact\n",
    "\n",
    "Edited by [Dalton Flanagan](https://www.linkedin.com/in/daltonflanagan/) (dalton@meta.com) with contributions from Mohsen Agsen, Bryce Bortree, Ricardo Juan Palma Duran, Kaolin Fire, Thomas Scialom."
   ]
  }
 ],
 "metadata": {
  "captumWidgetMessage": [],
  "dataExplorerConfig": [],
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "last_base_url": "https://bento.edge.x2p.facebook.net/",
  "last_kernel_id": "161e2a7b-2d2b-4995-87f3-d1539860ecac",
  "last_msg_id": "4eab1242-d815b886ebe4f5b1966da982_543",
  "last_server_session_id": "4a7b41c5-ed66-4dcb-a376-22673aebb469",
  "operator_data": [],
  "outputWidgetContext": []
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
